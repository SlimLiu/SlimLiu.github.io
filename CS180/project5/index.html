<!DOCTYPE html>
<html lang="en">
<head>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS180 Project 5: Fun with Diffusion Models!</title>
    <style>
        :root {
            --primary-font: "Times New Roman", Times, serif;
            --code-font: "Courier New", Courier, monospace;
            --max-width: 900px;
        }

        body {
            font-family: var(--primary-font);
            line-height: 1.6;
            color: #000;
            background-color: #fff;
            margin: 0;
            padding: 40px;
        }

        .container {
            max-width: var(--max-width);
            margin: 0 auto;
        }

        header {
            text-align: center;
            margin-bottom: 50px;
            border-bottom: 2px solid #000;
            padding-bottom: 20px;
        }

        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            font-weight: bold;
        }

        h2 {
            font-size: 1.8em;
            border-bottom: 1px solid #000;
            padding-bottom: 5px;
            margin-top: 50px;
        }

        h3 {
            font-size: 1.4em;
            margin-top: 30px;
            font-style: italic;
        }

        h4 {
            font-size: 1.2em;
            margin-top: 20px;
            font-weight: bold;
        }

        p {
            margin-bottom: 15px;
            text-align: justify;
        }

        .section {
            margin-bottom: 40px;
        }

        .image-gallery {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 20px;
            margin: 20px 0;
        }

        .image-item {
            text-align: center;
            margin-bottom: 20px;
        }

        img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ccc;
            display: block;
            margin: 0 auto;
        }

        .caption {
            margin-top: 8px;
            font-size: 0.9em;
            font-style: italic;
        }

        code {
            font-family: var(--code-font);
            background-color: #f4f4f4;
            padding: 2px 4px;
            font-size: 0.9em;
        }

        .todo {
            color: red;
            font-weight: bold;
            background-color: #ffe6e6;
            padding: 5px;
            border: 1px dashed red;
            display: block;
            margin: 10px 0;
            text-align: center;
        }

        /* Responsive adjustments */
        @media (max-width: 600px) {
            body {
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>CS180 Project 5: Fun with Diffusion Models!</h1>
            <p><strong>Name:</strong> Angxi Liu</p>
            <p><strong>Date:</strong> Fall 2025</p>
        </header>

        <section class="section">
            <h2>Part A: The Power of Diffusion Models!</h2>
            <p>In this part, I experimented with the DeepFloyd IF diffusion model, implemented sampling loops, and used them for various tasks including inpainting and optical illusions.</p>

            <h3>Part 0: Setup</h3>
            <p><strong>Random Seed:</strong> 100</p>
            
            <h4>Text Prompts and Embeddings</h4>
            
            <div class="image-gallery">
                <div class="image-item" style="max-width: 45%;">
                    <div style="display: flex; gap: 10px; justify-content: center;">
                        <img src="stage1_0_Neon-lit_alleyway_in.png" style="width: 128px; image-rendering: pixelated;" title="Stage 1 (64x64)">
                        <img src="stage2_0_Neon-lit_alleyway_in.png" style="width: 256px;" title="Stage 2 (256x256)">
                    </div>
                    <div class="caption">
                        <strong>Prompt 1:</strong> "Neon-lit alleyway in a futuristic metropolis, rain-slicked streets, chrome reflections, blurry figures with glowing implants. Cinematic lighting, high detail, volumetric fog."<br>
                        (Left: Stage 1, Right: Stage 2 | Steps: 20)
                    </div>
                </div>

                <div class="image-item" style="max-width: 45%;">
                    <div style="display: flex; gap: 10px; justify-content: center;">
                        <img src="stage1_1_Abandoned_robot_junk.png" style="width: 128px; image-rendering: pixelated;" title="Stage 1 (64x64)">
                        <img src="stage2_1_Abandoned_robot_junk.png" style="width: 256px;" title="Stage 2 (256x256)">
                    </div>
                    <div class="caption">
                        <strong>Prompt 2:</strong> "Abandoned robot junkyard under a perpetually overcast sky, rusting metal, scavengers picking through parts, overgrown weeds. Post-apocalyptic aesthetic, dramatic lighting."<br>
                        (Left: Stage 1, Right: Stage 2 | Steps: 20)
                    </div>
                </div>
            </div>

            <div class="image-gallery">
                <div class="image-item">
                    <div style="display: flex; gap: 20px; justify-content: center;">
                        <div>
                            <img src="stage2_2_A_flying_car_(hoverc.png" style="width: 256px;">
                            <div class="caption">Steps: 20</div>
                        </div>
                        <div>
                            <img src="stage2_more_steps_2_A_flying_car_(hoverc.png" style="width: 256px;">
                            <div class="caption">Steps: 40</div>
                        </div>
                    </div>
                    <div class="caption">
                        <strong>Prompt 3:</strong> "A flying car (hovercar) cruising above a dense urban sprawl, layered city architecture, visible power conduits, dusk light. Vibrant colors, detailed matte painting."<br>
                        (Comparison of Num Inference Steps)
                    </div>
                </div>
            </div>

            <p><strong>Reflection:</strong> 
                The DeepFloyd IF model demonstrates a profound capability in interpreting text semantics. In Prompt 1 and Prompt 2, the Stage 1 model (64x64), despite its lower resolution, accurately established the overall composition, tonality (e.g., the contrast of the neon lights, the somber palette of the junkyard), and the placement of core objects. The Stage 2 model effectively upscaled the low-resolution output; while preserving the original semantic structure, it significantly enhanced high-frequency details, such as the reflective texture of the "rain-slicked streets" and the fine details of the "rusting metal."
                <br><br>
                20 Steps (Lower Iteration): The image retains the overall composition and main color scheme but may appear slightly less coherent, blurrier, or blocky. Fine details, such as the texture of the rain-slicked streets or the intricate structure of the mechanical limbs, are less defined and more prone to visual noise or artifacts.

                40 Steps (Higher Iteration): There is a noticeable improvement in detail, sharpness, and texture definition. The increased denoising iterations allow the model to refine features, resulting in cleaner edges (e.g., on neon signs or architecture) and a more faithful representation of complex elements. The image quality is generally more polished and higher fidelity.
            </p>

            <h3>1.1 Implementing the Forward Process</h3>
            <p>The forward diffusion process $q(x_t | x_0)$ adds Gaussian noise to a clean image $x_0$ to obtain a noisy image $x_t$ at timestep $t$. This process is implemented based on the equation: 
$$ x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon $$
where $\epsilon \sim N(0, 1)$ and the variance is $\sigma_t^2 = 1 - \bar{\alpha}_t$.</p>
            
            <h4>Implementation of `forward` function</h4>
            <pre style="background-color: #f8f8f8; border: 1px solid #ccc; padding: 15px; overflow-x: auto;"><code>
def forward(im, t):
  """
  Args:
    im : torch tensor of size (1, 3, 64, 64) representing the clean image
    t : integer timestep

  Returns:
    im_noisy : torch tensor of size (1, 3, 64, 64) representing the noisy image at timestep t
  """
  with torch.no_grad():
    # ===== your code here! ====

    alpha_cumprod_t = stage_1.scheduler.alphas_cumprod[t]
    noise = torch.randn_like(im)
    im_noisy = torch.sqrt(alpha_cumprod_t) * im + torch.sqrt(1 - alpha_cumprod_t) * noise

    # ===== end of code ====
  return im_noisy
            </code></pre>

            <h4>Noisy Image Results</h4>
            <div class="image-gallery">
                <div class="image-item">
                    <img src="noise_level_250.png" alt="Campanile at t=250" style="width: 200px;">
                    <div class="caption">Campanile at t=250</div>
                </div>
                <div class="image-item">
                    <img src="noise_level_500.png" alt="Campanile at t=500" style="width: 200px;">
                    <div class="caption">Campanile at t=500</div>
                </div>
                <div class="image-item">
                    <img src="noise_level_750.png" alt="Campanile at t=750" style="width: 200px;">
                    <div class="caption">Campanile at t=750</div>
                </div>
            </div>

            <h3>1.2 Classical Denoising</h3>
            <p>To establish a baseline for the difficulty of denoising, we first applied a classical method: **Gaussian Blur filtering**. This technique involves convolving the image with a Gaussian kernel. The expected outcome is a trade-off: while the noise is attenuated, crucial image details are simultaneously lost (i.e., blurring of sharp edges).</p>

            <h4>Gaussian Blur Denoising Results</h4>
            <div class="image-gallery">
                <div class="image-item" style="max-width: 350px;">
                    <img src="gaussian_blur_t250.png" alt="Gaussian Blur Comparison at t=250" style="max-width: 100%; height: auto;">
                    <div class="caption">Comparison of the noisy Campanile image at $t=250$ with its Gaussian-denoised counterpart.</div>
                </div>
                <div class="image-item" style="max-width: 350px;">
                    <img src="gaussian_blur_t500.png" alt="Gaussian Blur Comparison at t=500" style="max-width: 100%; height: auto;">
                    <div class="caption">Comparison of the noisy Campanile image at $t=500$ with its Gaussian-denoised counterpart.</div>
                </div>
                <div class="image-item" style="max-width: 350px;">
                    <img src="gaussian_blur_t750.png" alt="Gaussian Blur Comparison at t=750" style="max-width: 100%; height: auto;">
                    <div class="caption">Comparison of the noisy Campanile image at $t=750$ with its Gaussian-denoised counterpart.</div>
                </div>
            </div>
            <p>As anticipated, the Gaussian filtering struggled to effectively remove the Gaussian noise without causing significant degradation of the structural integrity and high-frequency content. The difficulty increases substantially as the noise level $t$ rises (from 250 to 750), confirming the limitations of non-data-driven linear filtering methods for complex signal restoration tasks.</p>

            <h3>1.3 One-Step Denoising</h3>
            <p>Using the pretrained UNet to denoise in a single step.</p>
            <div class="image-gallery">
                <div class="image-item">
                    <img src="one-step-denoise.png" alt="One-Step Denoising Results" style="width: 100%; height: auto;">
                    <div class="caption">One-Step Denoising</div>
                </div>
            </div>

            <h3>1.4 Iterative Denoising</h3>
            <p>Using iterative sampling to improve results.</p>
            <div style="background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin-bottom: 20px; font-family: monospace; font-size: 0.9em; overflow-x: auto;">
<pre style="margin: 0;">
# Loop through timesteps
for i in range(i_start, len(timesteps) - 1):
    # 1. Calculate coefficients
    alpha_cumprod = alphas_cumprod[t]
    alpha = alpha_cumprod / alphas_cumprod[prev_t]
    beta = 1 - alpha

    # 2. Predict noise
    model_output = stage_1.unet(image, t, encoder_hidden_states=prompt_embeds)[0]
    noise_est, predicted_variance = torch.split(model_output, image.shape[1], dim=1)

    # 3. Compute predicted x_0 and posterior mean
    pred_x0 = (image - torch.sqrt(1 - alpha_cumprod) * noise_est) / torch.sqrt(alpha_cumprod)
    mean = (torch.sqrt(prev_alpha) * beta * pred_x0 + 
            torch.sqrt(alpha) * (1 - prev_alpha) * image) / (1 - alpha_cumprod)
    
    # 4. Update image
    image = add_variance(predicted_variance, t, mean)
</pre>
</div>

            <h4>1. Denoising Process (Intermediate Steps)</h4>
            <div class="image-gallery" style="display: flex; flex-wrap: wrap; justify-content: space-around; gap: 10px;">
                <div class="image-item" style="flex: 1; min-width: 150px; text-align: center;">
                    <img src="step_0_t690.png" alt="Step 0 (t=690)" style="width: 100%;">
                    <div class="caption">Step 0 (t=690)</div>
                </div>

                <div class="image-item" style="flex: 1; min-width: 150px; text-align: center;">
                    <img src="step_5_t540.png" alt="Step 5 (t=540)" style="width: 100%;">
                    <div class="caption">Step 5 (t=540)</div>
                </div>

                <div class="image-item" style="flex: 1; min-width: 150px; text-align: center;">
                    <img src="step_10_t390.png" alt="Step 10 (t=390)" style="width: 100%;">
                    <div class="caption">Step 10 (t=390)</div>
                </div>

                <div class="image-item" style="flex: 1; min-width: 150px; text-align: center;">
                    <img src="step_15_t240.png" alt="Step 15 (t=240)" style="width: 100%;">
                    <div class="caption">Step 15 (t=240)</div>
                </div>

                <div class="image-item" style="flex: 1; min-width: 150px; text-align: center;">
                    <img src="step_20_t90.png" alt="Step 20 (t=90)" style="width: 100%;">
                    <div class="caption">Step 20 (t=90)</div>
                </div>
            </div>

            <br>

            <h4>2. Final Results Comparison</h4>
            <div class="image-gallery" style="display: flex; justify-content: center;">
                <div class="image-item" style="text-align: center;">
                    <img src="iterative-camparison.png" alt="Iterative Comparison" style="max-width: 100%;">
                    <div class="caption">Iterative Denoising vs. Gaussian Blur & One-Step</div>
                </div>
            </div>

            <h3>1.5 Diffusion Model Sampling</h3>
            <p>Generated 5 sample images using "a high quality photo" prompt.</p>

            <div class="image-gallery" style="display: flex; justify-content: center;">
                <div class="image-item" style="text-align: center;">
                    <img src="sample1-5.png" alt="5 Generated Samples" style="max-width: 100%;">
                    <div class="caption">Samples 1-5 (Generated from pure noise)</div>
                </div>
            </div>

            <h3>1.6 Classifier-Free Guidance (CFG)</h3>
            <p>Using CFG scale of 7. Implementation details of the noise estimation:</p>

            <div style="background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin-bottom: 20px; font-family: monospace; font-size: 0.9em; overflow-x: auto;">
            <pre style="margin: 0;">
            # 1. Get conditional noise estimate
            model_output = stage_1.unet(image, t, encoder_hidden_states=prompt_embeds...)[0]

            # 2. Get unconditional noise estimate
            uncond_model_output = stage_1.unet(image, t, encoder_hidden_states=uncond_prompt_embeds...)[0]

            # 3. Split estimates
            noise_est, _ = torch.split(model_output, image.shape[1], dim=1)
            uncond_noise_est, _ = torch.split(uncond_model_output, image.shape[1], dim=1)

            # 4. Compute the CFG noise estimate (The Core Formula)
            cfg_noise_est = uncond_noise_est + scale * (noise_est - uncond_noise_est)
            </pre>
            </div>

            <p>Generated Results:</p>

            <div class="image-gallery" style="display: flex; justify-content: center;">
                <div class="image-item" style="text-align: center;">
                    <img src="cfg-sample1-5.png" alt="CFG Samples 1-5" style="max-width: 100%;">
                    <div class="caption">5 Samples Generated with CFG Scale = 7</div>
                </div>
            </div>
            <div>
                By applying Classifier-Free Guidance (CFG), we observe significant improvements in visual fidelity compared to standard sampling:
            </p>
            <ul>
                <li><strong>More Vivid Colors:</strong> The color saturation is noticeably enhanced, making the images look more vibrant.</li>
                <li><strong>Higher Contrast:</strong> CFG effectively removes the "washed-out" or "foggy" look, resulting in a wider dynamic range and stronger contrast between light and dark areas.</li>
            </ul>
            </div>

            <h3>1.7 Image-to-Image Translation (SDEdit)</h3>
            <p>SDEdit with "a high quality photo" prompt, applied to different starting noise levels.</p>

            <h4>Campanile Edits (Combined Series)</h4>
            <div class="image-gallery" style="display: flex; justify-content: center;">
                <div class="image-item" style="text-align: center;">
                    <img src="tower.png" alt="Campanile SDEdit Series (i_start 1-20)" style="max-width: 100%;">
                    <div class="caption">SDEdit results comparison across noise levels (i_start = 1, 3, 5, 7, 10, 20)</div>
                </div>
            </div>

            <h4>Own Image Edits</h4>
            <div class="image-gallery" style="display: flex; flex-wrap: wrap; justify-content: space-around; gap: 20px;">
                
                <div class="image-item" style="flex: 1; min-width: 300px; text-align: center;">
                    <img src="m1.png" alt="Custom Image 1 Series" style="max-width: 100%;">
                    <div class="caption">Image 1</div>
                </div>

                <div class="image-item" style="flex: 1; min-width: 300px; text-align: center;">
                    <img src="m2.png" alt="Custom Image 2 Series" style="max-width: 100%;">
                    <div class="caption">Image 2</div>
                </div>
            </div>

            <h4>1.7.1 Editing Hand-Drawn and Web Images</h4>
            <p>Comparison of SDEdit applied to a web image and hand-drawn sketches.</p>

            <div class="image-gallery" style="display: flex; flex-direction: column; gap: 30px;">
                
                <div class="image-item" style="text-align: center;">
                    <img src="haizeiwang.png" alt="Web Image Edits" style="max-width: 100%; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
                    <div class="caption">Web Image Edits</div>
                </div>

                <div class="image-item" style="text-align: center;">
                    <img src="handdrawn1.png" alt="Hand-Drawn 1 Edits" style="max-width: 100%; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
                    <div class="caption">Hand-Drawn Image 1 Edits</div>
                </div>

                <div class="image-item" style="text-align: center;">
                    <img src="handdrawn2.png" alt="Hand-Drawn 2 Edits" style="max-width: 100%; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
                    <div class="caption">Hand-Drawn Image 2 Edits</div>
                </div>

            </div>

            <h3>1.7.2 Inpainting</h3>
            <p>We implement inpainting by modifying the iterative denoising process. At each step, we force the pixels outside the mask to match the noisy version of the original image, while letting the model generate pixels inside the mask.</p>

            <div style="background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin-bottom: 25px; font-family: monospace; font-size: 0.9em; border: 1px solid #ddd; overflow-x: auto;">
            <pre style="margin: 0;">
# Inside the iterative loop (for each timestep t):
                
# 1. Compute the denoised step as usual (pred_prev_image)
# ... standard DDPM step ...

# 2. FORCING ORIGINAL PIXELS (The Inpainting Magic):
# Add noise to the original image to match the current timestep (prev_t)
original_noisy = forward(original_image, prev_t)

# 3. Combine them using the mask:
# mask=1: Keep the model's generation (The hole we want to fill)
# mask=0: Replace with the noisy original (The context we want to keep)
pred_prev_image = mask * pred_prev_image + (1 - mask) * original_noisy
            </pre>
            </div>

            <h4>Inpainting Results</h4>
            <div class="image-gallery" style="display: flex; flex-direction: column; gap: 30px;">
                
                <div class="image-item" style="text-align: center;">
                    <img src="mask1.png" alt="Campanile Inpainting" style="max-width: 60%; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                    <div class="caption"><strong>Campanile:</strong> Original | Mask | Inpainted Result</div>
                </div>

                <div class="image-item" style="text-align: center;">
                    <img src="mask2.png" alt="Custom Image 1 Inpainting" style="max-width: 60%; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                    <div class="caption"><strong>Custom Image 1:</strong> Original | Mask | Inpainted Result</div>
                </div>

                <div class="image-item" style="text-align: center;">
                    <img src="mask3.png" alt="Custom Image 2 Inpainting" style="max-width: 60%; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                    <div class="caption"><strong>Custom Image 2:</strong> Original | Mask | Inpainted Result</div>
                </div>

            </div>

            <h3>1.7.3 Text-Conditional Image-to-image Translation</h3>
            <p>Using SDEdit to modify images based on specific text prompts.</p>

            <div class="image-gallery" style="display: flex; flex-direction: column; gap: 40px;">

                <div class="image-item" style="text-align: center;">
                    <h4 style="margin-bottom: 15px;">Campanile Source Edits</h4>
                    <div style="display: flex; justify-content: center; gap: 20px; flex-wrap: wrap;">
                        
                        <div style="flex: 1; min-width: 250px;">
                            <img src="hotdog.png" alt="Campanile as Hotdog" style="width: 100%; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                            <div class="caption">Prompt: "a hotdog"</div>
                        </div>

                        <div style="flex: 1; min-width: 250px;">
                            <img src="skull.png" alt="Campanile as Skull" style="width: 100%; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                            <div class="caption">Prompt: "a spooky human skull"</div>
                        </div>
                        
                    </div>
                </div>

                <div class="image-item" style="text-align: center;">
                    <h4 style="margin-bottom: 10px;">Custom Image 1 Edit</h4>
                    <img src="blonde.png" alt="Custom Image 1 Edit" style="max-width: 60%; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                    <div class="caption">Prompt: "A blonde woman looking over her shoulder"</div>
                </div>

                <div class="image-item" style="text-align: center;">
                    <h4 style="margin-bottom: 10px;">Custom Image 2 Edit</h4>
                    <img src="scream.png" alt="Custom Image 2 Edit" style="max-width: 60%; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                    <div class="caption">Prompt: "a scary man who is screaming"</div>
                </div>

            </div>

            <h3>1.8 Visual Anagrams</h3>
            <p>Optical illusions created by denoising with different prompts for flipped and original orientations.</p>

            <h4>Implementation Details</h4>
            <div style="background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin-bottom: 30px; font-family: monospace; font-size: 0.85em; border: 1px solid #ddd; overflow-x: auto;">
            <pre style="margin: 0;">
            def make_flip_illusion(image, i_start, prompt1, prompt2, scale=7):
                # Loop through the denoising schedule
                for i in range(i_start, len(timesteps) - 1):
                    t = timesteps[i]
                    
                    # --- 1. Forward View (Prompt 1) ---
                    # Predict noise for the upright image
                    cond_out1 = unet(image, t, encoder_hidden_states=prompt1)
                    uncond_out1 = unet(image, t, encoder_hidden_states=uncond_prompt)
                    noise_1 = uncond_out1 + scale * (cond_out1 - uncond_out1)

                    # --- 2. Flipped View (Prompt 2) ---
                    # Flip image upside down (dim=2 is height)
                    flipped_image = torch.flip(image, [2])
                    
                    # Predict noise for the flipped image
                    cond_out2 = unet(flipped_image, t, encoder_hidden_states=prompt2)
                    uncond_out2 = unet(flipped_image, t, encoder_hidden_states=uncond_prompt)
                    noise_2_flipped = uncond_out2 + scale * (cond_out2 - uncond_out2)
                    
                    # Flip the noise back to align with original orientation
                    noise_2 = torch.flip(noise_2_flipped, [2])

                    # --- 3. Combine & Denoise ---
                    # Average the gradients from both views
                    final_noise = (noise_1 + noise_2) / 2
                    
                    # Standard DDPM Step
                    image = step(image, final_noise, t)
                    
                return image
            </pre>
            </div>

            <h4>Generated Illusions</h4>
            <div class="image-gallery" style="display: flex; flex-direction: column; gap: 40px;">

                <div class="image-item" style="text-align: center;">
                    <h5 style="margin-bottom: 10px;">Illusion 1: Old Man vs. Princess</h5>
                    <img src="flip1.png" alt="Illusion 1 Result" style="max-width: 70%; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                    <div class="caption">Prompt 1: "An oil painting of an old man"<br>Prompt 2: "An oil painting of a beautiful princess"</div>
                </div>

                <div class="image-item" style="text-align: center;">
                    <h5 style="margin-bottom: 10px;">Illusion 2: Skull vs. Victorian Lady</h5>
                    <img src="flip2.png" alt="Illusion 2 Result" style="max-width: 70%; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                    <div class="caption">Prompt 1: "A spooky human skull"<br>Prompt 2: "A portrait of a victorian lady in a dress"</div>
                </div>

            </div>

            <h3>1.9 Hybrid Images</h3>
<p>We use Factorized Diffusion to create hybrid images. By generating separate noise estimates for two different prompts and combining the low frequencies of one with the high frequencies of the other, we create an image that changes interpretation based on viewing distance.</p>

<h4>Core Implementation Logic</h4>
<p>The essential part inside the denoising loop where frequencies are mixed:</p>
<div style="background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin-bottom: 30px; font-family: monospace; font-size: 0.9em; border: 1px solid #ddd; overflow-x: auto;">
<pre style="margin: 0;">
#Inside the iterative denoising loop (at timestep t):

# 1. Get noise estimate for Prompt 1 (Low Frequency / Far View)
cfg_noise1 = uncond_noise + scale * (noise1 - uncond_noise)

# 2. Get noise estimate for Prompt 2 (High Frequency / Near View)
cfg_noise2 = uncond_noise + scale * (noise2 - uncond_noise)

# --- Frequency Mixing ---

# Extract LOW frequencies from noise1 using Gaussian blur
low_freq_noise = F.gaussian_blur(cfg_noise1, kernel_size=33, sigma=[2.0, 2.0])

# Extract HIGH frequencies from noise2 (Original minus Low-pass)
high_freq_noise = cfg_noise2 - F.gaussian_blur(cfg_noise2, kernel_size=33, sigma=[2.0, 2.0])

# Combine them to form the final noise estimate
final_noise = low_freq_noise + high_freq_noise

# Proceed with standard denoising step using final_noise...
</pre>
</div>

<h4>Generated Hybrid Results</h4>
<p>Try squinting or moving away from the screen to see the "Low Frequency" prompt, and look closely to see the "High Frequency" prompt.</p>

<div class="image-gallery" style="display: flex; flex-wrap: wrap; justify-content: center; gap: 30px;">
    
    <div class="image-item" style="flex: 1; min-width: 300px; max-width: 45%; text-align: center;">
        <img src="panda.png" alt="Hybrid Image 1" style="width: 100%; height: auto; border: 1px solid #eee; margin-bottom: 10px;">
        <div class="caption" style="text-align: left; background: #f9f9f9; padding: 10px; border-radius: 5px;">
            <strong>Hybrid 1:</strong><br>
            <strong>View from Far (Low Freq):</strong> "a photo of a giant panda"<br>
            <strong>View from Close (High Freq):</strong> "a snowy mountain with dark rocks"
        </div>
    </div>

    <div class="image-item" style="flex: 1; min-width: 300px; max-width: 45%; text-align: center;">
        <img src="burger.png" alt="Hybrid Image 2" style="width: 100%; height: auto; border: 1px solid #eee; margin-bottom: 10px;">
        <div class="caption" style="text-align: left; background: #f9f9f9; padding: 10px; border-radius: 5px;">
            <strong>Hybrid 2:</strong><br>
            <strong>View from Far (Low Freq):</strong> "a photo of a delicious burger"<br>
            <strong>View from Close (High Freq):</strong> "a grand canyon landscape"
        </div>
    </div>

</div>
</section>

        <section class="section" >
            <h2>Part B: Diffusion Models from Scratch!</h2>
            
            <h3>1. Training a Single-Step Denoising UNet</h3>
            
            <h4>1.2 Noising Process Visualization</h4>
            <p>Visualizing the noising process with σ = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0].</p>

            <div class="image-item">
                <img src="1.2.png" alt="Noising Process Visualization" style="max-width:100%; height:auto;">
            </div>


           <h4>1.2.1 Training the Denoiser (σ = 0.5)</h4>

            <!-- training loss 一行三图 -->
            <div class="image-gallery" style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center;">

                <div class="image-item" style="flex: 1 1 30%; text-align: center;">
                    <img src="1.2.1-loss1.png" alt="Training Loss Curve 1" style="width: 100%; height: auto;">
                    <div class="caption">Training Loss Curve (Run 1)</div>
                </div>

                <div class="image-item" style="flex: 1 1 30%; text-align: center;">
                    <img src="1.2.1-loss2.png" alt="Training Loss Curve 2" style="width: 100%; height: auto;">
                    <div class="caption">Training Loss Curve (Run 2)</div>
                </div>

                <div class="image-item" style="flex: 1 1 30%; text-align: center;">
                    <img src="1.2.1-loss3.png" alt="Training Loss Curve 3" style="width: 100%; height: auto;">
                    <div class="caption">Training Loss Curve (Run 3)</div>
                </div>

            </div>

            <!-- sample results -->
            <div class="image-gallery" style="margin-top: 1em; text-align: center;">

                <div class="image-item" style="display: inline-block; margin-right: 1em;">
                    <img src="1.2.1-e1.png" alt="Sample Results after Epoch 1" style="max-width: 100%; height: auto;">
                    <div class="caption">Sample Results on Test Set (Epoch 1, σ = 0.5)</div>
                </div>

                <div class="image-item" style="display: inline-block;">
                    <img src="1.2.1-e5.png" alt="Sample Results after Epoch 5" style="max-width: 100%; height: auto;">
                    <div class="caption">Sample Results on Test Set (Epoch 5, σ = 0.5)</div>
                </div>

            </div>



            <h4>1.2.2 Out-of-Distribution Testing</h4>
            <p>Testing the denoiser on noise levels it wasn't trained on.</p>

            <div class="image-item" style="text-align:center;">
                <img src="1.2.2.png" alt="Out-of-Distribution Results with Varying Sigma"
                    style="max-width:100%; height:auto;">
                <div class="caption">Out-of-Distribution Results (Varying σ)</div>
            </div>


            <h4>1.2.3 Denoising Pure Noise</h4>
            <p>Attempting to generate images by denoising pure Gaussian noise.</p>

            <div class="image-gallery" style="text-align:center;">

                <!-- loss curve -->
                <div class="image-item">
                    <img src="1.2.3-loss.png" alt="Training Loss Curve for Pure Noise Denoiser"
                        style="max-width:80%; height:auto;">
                    <div class="caption">Training Loss Curve (Pure Noise)</div>
                </div>

                <!-- sampling results -->
                <div class="image-gallery" style="display:flex; gap:20px; justify-content:center; margin-top:1em;">

                    <div class="image-item" style="flex:1 1 45%;">
                        <img src="1.2.3-e1.png" alt="Pure Noise Denoising Results after Epoch 1"
                            style="width:100%; height:auto;">
                        <div class="caption">Pure Noise Denoising Results (Epoch 1)</div>
                    </div>

                    <div class="image-item" style="flex:1 1 45%;">
                        <img src="1.2.3-e5.png" alt="Pure Noise Denoising Results after Epoch 5"
                            style="width:100%; height:auto;">
                        <div class="caption">Pure Noise Denoising Results (Epoch 5)</div>
                    </div>

                </div>
            </div>

            <p>
            <strong>Discussion:</strong>
            When trained to denoise pure Gaussian noise, the model consistently generates samples that resemble a single digit class, most notably digit “3”.
            This behavior can be explained by the use of an MSE loss: when the input contains no meaningful signal, the optimal prediction that minimizes the
            expected squared error is the conditional mean of the training data distribution.
            As a result, the denoiser collapses toward a prototypical or centroid-like image that represents an average over many training examples.
            Since digit “3” appears frequently and has a relatively smooth, symmetric structure, it dominates this average, causing most generated samples
            to converge toward a similar-looking digit.
            This highlights a limitation of simple denoising objectives when used as generative models without explicit multimodal or stochastic mechanisms.
            </p>


            <h3>2. Training a Flow Matching Model</h3>

            <h4>2.2 Time-Conditioned UNet</h4>

            <div class="image-item" style="text-align:center;">
                <img src="2.2-loss.png" alt="Training Loss Curve for Time-Conditioned UNet"
                    style="max-width:80%; height:auto;">
                <div class="caption">Training Loss Curve</div>
            </div>


            <h4>2.3 Sampling from Time-Conditioned UNet</h4>

            <!-- Epoch comparison -->
            <div class="image-gallery"
                style="display:flex; gap:20px; justify-content:center; flex-wrap:wrap; text-align:center;">

                <div class="image-item" style="flex:1 1 30%;">
                    <img src="2.3-e1.png" alt="Time-Conditioned UNet Samples at Epoch 1"
                        style="width:100%; height:auto;">
                    <div class="caption">Epoch 1</div>
                </div>

                <div class="image-item" style="flex:1 1 30%;">
                    <img src="2.3-e5.png" alt="Time-Conditioned UNet Samples at Epoch 5"
                        style="width:100%; height:auto;">
                    <div class="caption">Epoch 5</div>
                </div>

                <div class="image-item" style="flex:1 1 30%;">
                    <img src="2.3-e10.png" alt="Time-Conditioned UNet Samples at Epoch 10"
                        style="width:100%; height:auto;">
                    <div class="caption">Epoch 10</div>
                </div>
            </div>

            <p>
            As training progresses, the time-conditioned UNet generates samples that are progressively
            cleaner and more structured.
            At early epochs, the model struggles to remove noise and the samples appear blurry with
            weak semantic structure.
            By Epoch 10, the generated images exhibit clearer digit-like shapes, indicating that the
            model has learned to leverage timestep conditioning to perform coarse-to-fine denoising.
            This behavior aligns with the intuition behind diffusion models, where information is
            gradually recovered as noise levels decrease.
            </p>

            <!-- Seed comparison -->
            <div class="image-gallery"
                style="display:flex; gap:20px; justify-content:center; flex-wrap:wrap; text-align:center; margin-top:20px;">

                <div class="image-item" style="flex:1 1 30%;">
                    <img src="2.3-seed0.png" alt="Time-Conditioned UNet Samples (Seed 0)"
                        style="width:100%; height:auto;">
                    <div class="caption">Seed 0</div>
                </div>

                <div class="image-item" style="flex:1 1 30%;">
                    <img src="2.3-seed5.png" alt="Time-Conditioned UNet Samples (Seed 5)"
                        style="width:100%; height:auto;">
                    <div class="caption">Seed 5</div>
                </div>

                <div class="image-item" style="flex:1 1 30%;">
                    <img src="2.3-seed10.png" alt="Time-Conditioned UNet Samples (Seed 10)"
                        style="width:100%; height:auto;">
                    <div class="caption">Seed 10</div>
                </div>
            </div>

            <p>
            Using different random seeds while keeping the model checkpoint fixed results in visually
            distinct samples, demonstrating that the time-conditioned UNet is able to generate diverse
            outputs rather than collapsing to a single prototype.
            Although the overall digit quality and structure remain similar across seeds, fine-grained
            details such as stroke thickness and local shapes vary.
            This indicates that the model has learned a stochastic generative process conditioned on
            the noise initialization, consistent with the intended behavior of diffusion-style models.
            </p>

            <p><strong>Discussion.</strong>
            The two sets of visualizations highlight complementary aspects of the time-conditioned
            model.
            Varying the training epoch reveals how sample quality improves as the model learns to
            denoise more effectively over time, while varying the random seed illustrates the inherent
            stochasticity and diversity of the generative process.
            Together, these results suggest that the model successfully balances convergence and
            diversity, producing increasingly structured samples without collapsing to deterministic
            outputs.
            </p>



            </div>

            <h4>2.5 Class-Conditioned UNet</h4>

            <div class="image-item" style="text-align:center;">
                <img src="2.5-loss.png" alt="Training Loss Curve for Class-Conditioned UNet"
                    style="max-width:80%; height:auto;">
                <div class="caption">Training Loss Curve</div>
            </div>


            <h4>2.6 Sampling from Class-Conditioned UNet</h4>

            <div class="image-row" style="display:flex; gap:20px; justify-content:center; align-items:flex-start;">

                <div class="image-item" style="text-align:center; flex:1;">
                    <img src="2.6-1.png" alt="Epoch 1 Class-Conditioned Sampling"
                        style="width:100%; height:auto;">
                    <div class="caption">
                        <strong>Epoch 1</strong><br>
                        Each row corresponds to a digit class (0–9), with 4 instances per digit.
                        Samples show coarse digit structures with limited clarity.
                    </div>
                </div>

                <div class="image-item" style="text-align:center; flex:1;">
                    <img src="2.6-2.png" alt="Epoch 5 Class-Conditioned Sampling"
                        style="width:100%; height:auto;">
                    <div class="caption">
                        <strong>Epoch 5</strong><br>
                        Digit shapes become more recognizable, and class semantics are largely correct,
                        though some blur remains.
                    </div>
                </div>

                <div class="image-item" style="text-align:center; flex:1;">
                    <img src="2.6-3.png" alt="Epoch 10 Class-Conditioned Sampling"
                        style="width:100%; height:auto;">
                    <div class="caption">
                        <strong>Epoch 10</strong><br>
                        Samples exhibit clear digit structures and strong alignment with target classes,
                        demonstrating fast convergence enabled by class conditioning.
                    </div>
                </div>

            </div>

            <p>
            The class-conditioned UNet generates samples that are clearly aligned with the target
            digit labels while maintaining diversity within each class.
            As training progresses from Epoch 1 to Epoch 10, digit structures become increasingly
            well-defined, highlighting the effectiveness of class conditioning for accelerated
            convergence.
            </p>

            <p><strong>Scheduler Removal Experiment:</strong></p>

            <div class="image-item" style="text-align:center;">
                <img src="2.5-loss-nosched.png" alt="Training Loss without Scheduler"
                    style="max-width:80%; height:auto;">
                <div class="caption">
                    Training loss curve of the class-conditioned flow matching model
                    without using a learning rate scheduler.
                </div>
            </div>

            <p>
            To simplify the training pipeline, we remove the exponential learning rate scheduler
            during training and instead use a fixed learning rate.
            To compensate for the lack of learning rate decay, we reduce the base learning rate
            and apply gradient clipping to stabilize optimization.
            As shown in the loss curve, the model still converges rapidly and maintains stable
            training behavior, indicating that class conditioning enables effective learning
            even without a scheduler.
            </p>




            <h3>3. Bells & Whistles</h3>

            <p><strong>Better Time-Conditioned UNet (Fashion-MNIST):</strong></p>

            <div class="image-item" style="text-align:center;">
                <img src="3.png" alt="Fashion-MNIST Training Loss"
                    style="max-width:80%; height:auto;">
                <div class="caption">
                    Training loss curve for the improved time-conditioned UNet trained on Fashion-MNIST.
                    The model converges stably despite the increased complexity of clothing categories.
                </div>
            </div>

            <div class="image-gallery"
                style="display:flex; gap:20px; justify-content:center; flex-wrap:wrap; text-align:center; margin-top:20px;">

                <div class="image-item" style="flex:1 1 30%;">
                    <img src="3-e1.png" alt="Fashion-MNIST Sampling Epoch 1"
                        style="width:100%; height:auto;">
                    <div class="caption"><strong>Epoch 1</strong></div>
                </div>

                <div class="image-item" style="flex:1 1 30%;">
                    <img src="3-e2.png" alt="Fashion-MNIST Sampling Epoch 5"
                        style="width:100%; height:auto;">
                    <div class="caption"><strong>Epoch 5</strong></div>
                </div>

                <div class="image-item" style="flex:1 1 30%;">
                    <img src="3-e3.png" alt="Fashion-MNIST Sampling Epoch 10"
                        style="width:100%; height:auto;">
                    <div class="caption"><strong>Epoch 10</strong></div>
                </div>
            </div>

            <p>
            Using Fashion-MNIST introduces more complex visual structures compared to digit images,
            making time-conditioned generation more challenging.
            As training progresses from Epoch 1 to Epoch 10, the model gradually learns to recover
            coarse shapes such as shoes, clothing outlines, and bags, demonstrating improved
            noise removal and structural consistency over time.
            </p>

            <div class="image-item" style="text-align:center; margin-top:20px;">
                <img src="progression.png" alt="Fashion-MNIST Sampling Progression"
                    style="max-width:100%; height:auto;">
                <div class="caption">
                    Sampling progression visualization showing intermediate denoising states
                    from early noisy steps to the final generated Fashion-MNIST sample.
                </div>
            </div>

            <p><strong>Other Extensions:</strong></p>

            <p>
            Beyond digit datasets, we explore Fashion-MNIST as a more challenging benchmark to
            evaluate the generalization of the time-conditioned UNet.
            The progression visualization highlights the coarse-to-fine denoising behavior,
            where global structure emerges early and finer details are gradually refined.
            This extension demonstrates the flexibility of the model and its ability to scale
            to visually richer datasets without architectural changes.
            </p>

        </section>
    </div>
</body>
</html>